---
id: 5b2c3d4e-6f7a-8b9c-0d1e-2f3a4b5c6d7e
blueprint: code_snippet
title: 'Python Async Performance Optimization'
description: 'High-performance Python patterns using asyncio, concurrent processing, and memory optimization techniques for handling thousands of concurrent operations.'
language: python
programming_languages:
  - python
difficulty: advanced
use_cases:
  - async-programming
  - concurrency
  - performance-optimization
  - web-scraping
is_featured: false
external_link: null
status: published
date: 1705881600
---

```python
import asyncio
import aiohttp
import time
from functools import wraps
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any
import weakref

# 1. Async batch processing with semaphore control
class AsyncBatchProcessor:
    def __init__(self, max_concurrent: int = 10, batch_size: int = 100):
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.batch_size = batch_size
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=100, limit_per_host=20)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def process_item(self, item: Dict[str, Any]) -> Dict[str, Any]:
        async with self.semaphore:
            try:
                async with self.session.get(item['url']) as response:
                    data = await response.json()
                    return {'id': item['id'], 'result': data, 'status': 'success'}
            except Exception as e:
                return {'id': item['id'], 'error': str(e), 'status': 'error'}
    
    async def process_batch(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        tasks = [self.process_item(item) for item in items]
        return await asyncio.gather(*tasks, return_exceptions=True)
    
    async def process_all(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        results = []
        for i in range(0, len(items), self.batch_size):
            batch = items[i:i + self.batch_size]
            batch_results = await self.process_batch(batch)
            results.extend(batch_results)
            # Small delay to prevent overwhelming the server
            await asyncio.sleep(0.1)
        return results

# 2. Memory-efficient async generator for large datasets
async def process_large_dataset(data_source: str) -> AsyncIterator[Dict[str, Any]]:
    """Process large datasets without loading everything into memory"""
    async with aiohttp.ClientSession() as session:
        async with session.get(data_source, headers={'Accept': 'application/json'}) as response:
            async for line in response.content:
                try:
                    item = json.loads(line)
                    # Process item
                    processed = await expensive_processing(item)
                    yield processed
                except json.JSONDecodeError:
                    continue

async def expensive_processing(item: Dict[str, Any]) -> Dict[str, Any]:
    """Simulate CPU-intensive processing"""
    await asyncio.sleep(0.01)  # Simulate async I/O
    return {
        'processed_id': item.get('id'),
        'computed_value': sum(item.get('values', [])),
        'timestamp': time.time()
    }

# 3. Async caching decorator with TTL
class AsyncLRUCache:
    def __init__(self, maxsize: int = 128, ttl: float = 3600):
        self.cache = {}
        self.maxsize = maxsize
        self.ttl = ttl
        self.access_order = []
    
    def __call__(self, func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            key = str(args) + str(sorted(kwargs.items()))
            now = time.time()
            
            # Check if cached and not expired
            if key in self.cache:
                value, timestamp = self.cache[key]
                if now - timestamp < self.ttl:
                    # Move to end (most recently used)
                    self.access_order.remove(key)
                    self.access_order.append(key)
                    return value
                else:
                    # Expired, remove from cache
                    del self.cache[key]
                    self.access_order.remove(key)
            
            # Execute function
            result = await func(*args, **kwargs)
            
            # Add to cache
            if len(self.cache) >= self.maxsize:
                # Remove least recently used
                oldest_key = self.access_order.pop(0)
                del self.cache[oldest_key]
            
            self.cache[key] = (result, now)
            self.access_order.append(key)
            
            return result
        return wrapper

# Usage example
@AsyncLRUCache(maxsize=256, ttl=1800)
async def fetch_user_data(user_id: int) -> Dict[str, Any]:
    async with aiohttp.ClientSession() as session:
        async with session.get(f'/api/users/{user_id}') as response:
            return await response.json()

# 4. Connection pooling and resource management
class OptimizedAsyncClient:
    def __init__(self):
        self.connector = aiohttp.TCPConnector(
            limit=100,              # Total connection pool size
            limit_per_host=20,      # Max connections per host
            ttl_dns_cache=300,      # DNS cache TTL
            use_dns_cache=True,
            keepalive_timeout=60,   # Keep connections alive
            enable_cleanup_closed=True
        )
        self.timeout = aiohttp.ClientTimeout(
            total=30,
            connect=10,
            sock_read=10
        )
        self.session = None
    
    async def start(self):
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=self.timeout
        )
    
    async def close(self):
        if self.session:
            await self.session.close()
    
    async def get(self, url: str, **kwargs) -> Dict[str, Any]:
        async with self.session.get(url, **kwargs) as response:
            return await response.json()

# 5. CPU-bound task optimization with asyncio
class AsyncCPUOptimizer:
    def __init__(self, max_workers: int = None):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    async def run_cpu_bound(self, func, *args, **kwargs):
        """Run CPU-bound function in thread pool"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, func, *args, **kwargs)
    
    async def parallel_cpu_tasks(self, tasks: List[tuple]) -> List[Any]:
        """Run multiple CPU-bound tasks in parallel"""
        futures = []
        for func, args, kwargs in tasks:
            future = self.run_cpu_bound(func, *args, **kwargs)
            futures.append(future)
        
        return await asyncio.gather(*futures)
    
    def __del__(self):
        self.executor.shutdown(wait=False)

# Example usage
async def main():
    # Batch processing example
    items = [{'id': i, 'url': f'https://api.example.com/item/{i}'} 
             for i in range(1000)]
    
    async with AsyncBatchProcessor(max_concurrent=20) as processor:
        results = await processor.process_all(items)
        print(f"Processed {len(results)} items")
    
    # Memory-efficient processing
    async for processed_item in process_large_dataset('https://api.example.com/large-dataset'):
        # Handle each item as it's processed
        print(f"Processed item: {processed_item['processed_id']}")

if __name__ == "__main__":
    asyncio.run(main())
```